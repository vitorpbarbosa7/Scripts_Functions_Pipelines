lgb.LGBMClassifier(
    boosting_type='gbdt',
    num_leaves=31,
    max_depth=-1,
    learning_rate=0.1,
    n_estimators=100,
    subsample_for_bin=200000,
    objective=None,
    class_weight=None,
    min_split_gain=0.0,
    min_child_weight=0.001,
    min_child_samples=20,
    subsample=1.0,
    subsample_freq=0,
    colsample_bytree=1.0,
    reg_alpha=0.0,
    reg_lambda=0.0,
    random_state=None,
    n_jobs=-1,
    silent=True,
    importance_type='split',
    **kwargs,
)
Docstring:      LightGBM classifier.
Init docstring:
Construct a gradient boosting model.

Parameters
----------
boosting_type : string, optional (default='gbdt')
    'gbdt', traditional Gradient Boosting Decision Tree.
    'dart', Dropouts meet Multiple Additive Regression Trees.
    'goss', Gradient-based One-Side Sampling.
    'rf', Random Forest.
num_leaves : int, optional (default=31)
    Maximum tree leaves for base learners.
max_depth : int, optional (default=-1)
    Maximum tree depth for base learners, <=0 means no limit.
learning_rate : float, optional (default=0.1)
    Boosting learning rate.
    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate
    in training using ``reset_parameter`` callback.
    Note, that this will ignore the ``learning_rate`` argument in training.
n_estimators : int, optional (default=100)
    Number of boosted trees to fit.
subsample_for_bin : int, optional (default=200000)
    Number of samples for constructing bins.
objective : string, callable or None, optional (default=None)
    Specify the learning task and the corresponding learning objective or
    a custom objective function to be used (see note below).
    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.
class_weight : dict, 'balanced' or None, optional (default=None)
    Weights associated with classes in the form ``{class_label: weight}``.
    Use this parameter only for multi-class classification task;
    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.
    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.
    You may want to consider performing probability calibration
    (https://scikit-learn.org/stable/modules/calibration.html) of your model.
    The 'balanced' mode uses the values of y to automatically adjust weights
    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.
    If None, all classes are supposed to have weight one.
    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)
    if ``sample_weight`` is specified.
min_split_gain : float, optional (default=0.)
    Minimum loss reduction required to make a further partition on a leaf node of the tree.
min_child_weight : float, optional (default=1e-3)
    Minimum sum of instance weight (hessian) needed in a child (leaf).
min_child_samples : int, optional (default=20)
    Minimum number of data needed in a child (leaf).
subsample : float, optional (default=1.)
    Subsample ratio of the training instance.
subsample_freq : int, optional (default=0)
    Frequence of subsample, <=0 means no enable.
colsample_bytree : float, optional (default=1.)
    Subsample ratio of columns when constructing each tree.
reg_alpha : float, optional (default=0.)
    L1 regularization term on weights.
reg_lambda : float, optional (default=0.)
    L2 regularization term on weights.
random_state : int, RandomState object or None, optional (default=None)
    Random number seed.
    If int, this number is used to seed the C++ code.
    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.
    If None, default seeds in C++ code are used.
n_jobs : int, optional (default=-1)
    Number of parallel threads.
silent : bool, optional (default=True)
    Whether to print messages while running boosting.
importance_type : string, optional (default='split')
    The type of feature importance to be filled into ``feature_importances_``.
    If 'split', result contains numbers of times the feature is used in a model.
    If 'gain', result contains total gains of splits which use the feature.
**kwargs
    Other parameters for the model.
    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.

    .. warning::

        \*\*kwargs is not supported in sklearn, it may cause unexpected issues.

Note
----
A custom objective function can be provided for the ``objective`` parameter.
In this case, it should have the signature
``objective(y_true, y_pred) -> grad, hess`` or
``objective(y_true, y_pred, group) -> grad, hess``:

    y_true : array-like of shape = [n_samples]
        The target values.
    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)
        The predicted values.
    group : array-like
        Group/query data.
        Only used in the learning-to-rank task.
        sum(group) = n_samples.
        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,
        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.
    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)
        The value of the first order derivative (gradient) for each sample point.
    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)
        The value of the second order derivative (Hessian) for each sample point.

For binary task, the y_pred is margin.
For multi-class task, the y_pred is group by class_id first, then group by row_id.
If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]
and you should group grad and hess in this way as well.
File:           ~/anaconda3/lib/python3.7/site-packages/lightgbm/sklearn.py
Type:           type
Subclasses:     DaskLGBMClassifier